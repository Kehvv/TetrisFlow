{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##References##\n",
    "##https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5\n",
    "##https://www.udemy.com/course/deep-learning-and-computer-vision/\n",
    "##https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8\n",
    "##https://www.pyimagesearch.com/2017/08/21/deep-learning-with-opencv/\n",
    "##https://keras.io/api/layers/core_layers/\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json ##load model created earlier\n",
    "model = model_from_json(open(\"facial_expression_model_structure.json\", \"r\").read())\n",
    "model.load_weights('facial_expression_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_category = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral') ## defining the emotion categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'emotion.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3d1f57cac824>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcapRead\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memotion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxpos\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mypos\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## image display\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'emotion.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                       \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_emotion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'emotion.txt'"
     ]
    }
   ],
   "source": [
    "cameraCap = cv2.VideoCapture(0) ## using the built in open cv video capture using webcam\n",
    "frame = 0\n",
    "\n",
    "\n",
    "while(True): ## continuously read the frames of the video like an image\n",
    "    \n",
    "    ret, capRead = cameraCap.read() ## ret returns 1 if video captured\n",
    "    capRead = cv2.resize(capRead, (640, 360)) ##resize captured video to this dimension\n",
    "    capRead = capRead[0:308,:] \n",
    "    vidBW = cv2.cvtColor(capRead, cv2.COLOR_BGR2GRAY) ##convert loaded video frame to grayscale\n",
    "    faces = face_cascade.detectMultiScale(vidBW, 1.3, 5) ##detect face in loaded gray frame using cascade classifier provided by OpenCV\n",
    "\n",
    "    for (xpos,ypos,w,h) in faces:\n",
    "        if w > 130: ## only detect images over this width\n",
    "            cv2.rectangle(capRead,(xpos,ypos),(xpos+w,ypos+h),(0,255,0),2) ##draw rectangle around detected face using coordinates\n",
    "            \n",
    "            face_detected = capRead[int(ypos):int(ypos+h), int(xpos):int(xpos+w)] ##convert coordinates to int\n",
    "            face_detected = cv2.cvtColor(face_detected, cv2.COLOR_BGR2GRAY) ##convert to grayscale\n",
    "            face_detected = cv2.resize(face_detected, (48, 48)) ##must resize to 48x48 pixels\n",
    "\n",
    "            img_pixels = image.img_to_array(face_detected) ## add to array\n",
    "            img_pixels = np.expand_dims(img_pixels, axis = 0) ##expand the dimensions\n",
    "            img_pixels /= 255 ##normalise \n",
    "\n",
    "            predict = model.predict(img_pixels) ##storing predictions of each emotion\n",
    "            top_emotion = np.argmax(predict[0]) ##take highest probability prediction\n",
    "\n",
    "            overlay = capRead.copy() ##overlay to store prediction text\n",
    "            opacity = 0.7 \n",
    "            \n",
    "            cv2.rectangle(capRead,(xpos+w+10,ypos-25),(xpos+w+150,ypos+115),(64,64,64),cv2.FILLED) ##adds an offset rectangle to show emotion list\n",
    "            cv2.addWeighted(overlay, opacity, capRead, 1 - opacity, 0, capRead) \n",
    "            cv2.line(capRead,(int((xpos+xpos+w)/2),ypos+15),(xpos+w,ypos-20),(255,255,255),1) ##line that points to detected face\n",
    "            cv2.line(capRead,(xpos+w,ypos-20),(xpos+w+10,ypos-20),(255,255,255),1)\n",
    "            emotion = \"\" ##initialise as blank string\n",
    "            \n",
    "            for i in range(len(predict[0])): \n",
    "                emotion = \"%s %s%s\" % (emotion_category[i], round(predict[0][i]*100, 2), '%') ##output emotion and prediction percentage\n",
    "                color = (255,255,255)\n",
    "                cv2.putText(capRead, emotion, (int(xpos+w+15), int(ypos-12+i*20)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1) ## image display\n",
    "                \n",
    "                with open('emotion.txt', 'w') as f:\n",
    "                      f.write(str(top_emotion))   \n",
    "\n",
    "    cv2.imshow('TetrisFlow Capture',capRead)\n",
    "    frame = frame + 1 ##continuously increasing frame\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): #press q to quit\n",
    "        break\n",
    "\n",
    "cameraCap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
